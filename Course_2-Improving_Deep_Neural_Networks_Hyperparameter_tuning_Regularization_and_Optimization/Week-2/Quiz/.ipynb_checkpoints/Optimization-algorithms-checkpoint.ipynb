{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Which notation would you use to denote the 3rd layer’s activations when the input is the 7th example from the 8th minibatch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: ```a^[3]{8}(7)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Which of these statements about mini-batch gradient descent do you agree with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Why is the best mini-batch size usually not 1 and not m, but instead something in-between?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch.\n",
    "- If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Suppose your learning algorithm’s cost JJJ, plotted as a function of the number of iterations, looks like this: \n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/KIycr3grEeeJIwrF5BVsIg_f1c324824bd9220c7ee985cce1521404_cost.png?expiry=1560902400000&amp;hmac=HLMjTfp_MJB73WfRl-m_yjFjk789wT4QpeoHIL1x-8o\" alt=\"\">\n",
    "#### Which of the following do you agree with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Suppose the temperature in Casablanca over the first three days of January are the same:\n",
    "\n",
    "Jan 1st: ```θ1=10 C```\n",
    "\n",
    "Jan 2nd: ```θ2=10 C```\n",
    "(We used Fahrenheit in lecture, so will use Celsius here in honor of the metric world.)\n",
    "\n",
    "#### Say you use an exponentially weighted average with ```β=0.5``` to track the temperature: ```v0=0, v_t=βv_t−1+(1−β)θ_t```. If ```v2``` is the value computed after day 2 without bias correction, and ```v2_corrected``` is the value you compute with bias correction. What are these values? (You might be able to do this without a calculator, but you don't actually need one. Remember what is bias correction doing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: ```v2=7.5, v2_corrected=10```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Which of these is NOT a good learning rate decay scheme? Here, t is the epoch number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: ```α=etα0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. You use an exponentially weighted average on the London temperature dataset. You use the following to track the temperature: ```v_t=βv_t−1+(1−β)θ_t```. The red line below was computed using ```β=0.9```. What would happen to your red curve as you vary ```β```? (Check the two that apply)\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/W0boqHgrEee6mw7xN92yoA_3a1f4052dc56969b5d7da4024a46836d_temp.png?expiry=1560902400000&amp;hmac=wsq-fsleUXudhVySvM4W6OpPDz1V4LblhRK_8_9qAyQ\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- Increasing β will shift the red line slightly to the right.\n",
    "- Decreasing β will create more oscillation within the red line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.  Consider this figure:\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/fv6gungsEeeJIwrF5BVsIg_6da8c45ffcd4075de23d8e93884937f1_GD.png?expiry=1560902400000&amp;hmac=axAMMv22q_HWU7wMGhYRWhSYnk0M5s0Lp9fV6NcD19Y\" alt=\"\">\n",
    "#### These plots were generated with gradient descent; with gradient descent with momentum ``` (β = 0.5) ``` and gradient descent with momentum ``` (β = 0.9) ```. Which curve corresponds to which algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: (1) is gradient descent. (2) is gradient descent with momentum ```(small β)```. (3) is gradient descent with momentum ```(large β)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Suppose batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function ``` J(W[1],b[1],...,W[L],b[L]). ``` Which of the following techniques could help find parameter values that attain a small value forJ? (Check all that apply) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- Try mini-batch gradient descent\n",
    "- Try tuning the learning rate α\\alphaα\n",
    "- Try using Adam\n",
    "- Try better random initialization for the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Which of the following statements about Adam is False?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Adam should be used with batch gradient computations, not with mini-batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
